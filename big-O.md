# Big O Notation
Big O Notation is one of the most necessary mathematical notations used in computer science to measure an algorithm's efficiency. We can analyze how efficient an algorithm is from the amount of time, storage, and other resources it takes to run the algorithm and a change in the input size. Big O Notation in Data Structure tells us how well an algorithm will perform in a particular situation. In other words, it gives an algorithm's upper-bound runtime or worst-case complexity. 


Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. Big O is a member of a family of notations invented by German mathematicians Paul Bachmann, Edmund Landau, and others, collectively called Bachmannâ€“Landau notation or asymptotic notation. The letter O was chosen by Bachmann to stand for Ordnung, meaning the order of approximation. 
## References
1.<https://www.simplilearn.com/big-o-notation-in-data-structure-article>
2.<https://en.wikipedia.org/wiki/Big_O_notation>